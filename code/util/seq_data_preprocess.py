import sys, os, argparse
parentdir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parentdir)
import random
import torch
import pickle
import gzip
import os
import math
import multiprocessing
from tqdm import tqdm
import numpy as np
from transformers import BertTokenizer, BertModel, BertConfig
from util.utils import set_seed, load_embedding
set_seed()



def generate_cl_listpair(k, pos_k, T5_dir):
    set_seed()
    data_dir = '../data/attn_data/best_rank/'
    data_dir2 = '../data/attn_data/div_query.data'
    save_dir = '../data/attn_data/same_subtopic_docs_T5.pkl'
    files = os.listdir(data_dir)
    generate_cl_listpair_sub(k, data_dir2)
    find_same_subtopic_docs_T5(files, data_dir, T5_dir, save_dir)
    generate_cl_listpair_sub_pos(pos_k, save_dir=save_dir)

def generate_cl_listpair_sub(k, data_dir):
    '''for each query, random shuffle the candidate document list and obtain 50 document sequence'''
    set_seed()
    doc_set = {}
    cl_pairl = {}
    qd = pickle.load(open(data_dir, 'rb'))
    for qid in tqdm(qd):
        doc_list = qd[qid].best_docs_rank  # top 50
        """
        ['DOC_NUM', 'add_cl_pair', 'add_docs', 'add_docs_rel_score', 'add_query_suggestion', 'best_docs_rank',
        'best_docs_rank_rel_score', 'best_gain', 'best_metric', 'best_subtopic_df', 'doc_list', 'doc_score_list',
        'get_alpha_DCG', 'get_best_rank', 'get_test_alpha_nDCG', 'qid', 'query', 'query_suggestion',
        'set_std_metric', 'stand_alpha_DCG', 'subtopic_df', 'subtopic_id_list', 'subtopic_list']
        """
        tmp = []
        for i in range(k):
            id = list(np.arange(len(doc_list)))
            random.shuffle(id)
            doc_list_new = [doc_list[_id] for _id in id]
            tmp.append(doc_list_new)
        doc_set[qd[qid].query] = list(set(sum(tmp, [])))
        cl_pairl[qd[qid].query] = (qid, doc_list, tmp)
    torch.save(doc_set, '../data/attn_data/doc_set.pkl')
    torch.save(cl_pairl, '../data/attn_data/cl_pairl.pkl')

def find_same_subtopic_docs_T5(files, data_dir, T5_dir, save_dir):
    '''find documents covering same subtopics generated by T5'''
    set_seed()

    doc_id_list = torch.load(T5_dir + 'doc_id_list.pkl')
    similarity = torch.load(T5_dir + 'doc_cos_similarity.pkl')

    result = {}
    for file in tqdm(files):
        data = pickle.load(open(data_dir + file, 'rb'))
        result[data.query] = [data.qid]
        bdr = data.best_docs_rank
        bdr_index = [doc_id_list.index(val) for val in bdr]

        doc_d = {}
        for i in range(len(bdr)-1):
            doc_i = bdr_index[i]
            for j in range(i+1, len(bdr)):
                doc_j = bdr_index[j]
                if similarity[doc_i][doc_j] > 0.99:
                    if bdr[i] not in doc_d:
                        doc_d[bdr[i]] = []
                    if bdr[j] not in doc_d:
                        doc_d[bdr[j]] = []
                    if bdr[j] not in doc_d[bdr[i]]:
                        doc_d[bdr[i]].append(bdr[j])
                    if bdr[i] not in doc_d[bdr[j]]:
                        doc_d[bdr[j]].append(bdr[i])
        result[data.query].append(doc_d)
    torch.save(result, save_dir)

def generate_cl_listpair_sub_pos(pos_k, save_dir='../data/attn_data/same_subtopic_docs.pkl'):
    '''generate document sequence pairs'''
    set_seed()
    same_subtopic_docs = torch.load(save_dir)
    cl_pairl = torch.load('../data/attn_data/cl_pairl.pkl')
    result = {}
    count = 0
    for query in tqdm(cl_pairl):
        result[cl_pairl[query][0]] = [query]
        best_doc_lists = cl_pairl[query][2]

        for best_doc_list in best_doc_lists:
            exchange_doc_list = doc_change(best_doc_list)
            count += 1
            for i in range(len(best_doc_list)):
                doc_id = best_doc_list[i]
                for j in range(1, len(same_subtopic_docs[query]), 1):
                    if doc_id not in same_subtopic_docs[query][j]:
                        continue
                    sample_id = random.sample([val for val in range(len(same_subtopic_docs[query][j][doc_id]))],
                                              min(pos_k, len(same_subtopic_docs[query][j][doc_id])))
                    for k in sample_id:
                        pos_pair = best_doc_list[:i]+[same_subtopic_docs[query][j][doc_id][k]]+best_doc_list[i+1:]
                        result[cl_pairl[query][0]].append([best_doc_list, pos_pair, exchange_doc_list])
                        count += 1
    torch.save(result, '../data/attn_data/cl_pair_pos_'+str(pos_k)+'.pkl')
    print('TOTAL DOC NUMBER: ', count)


def generate_cll_train_data(bert_model_path, max_seq_length, tmp_dir, T5_dir):
    set_seed()
    doc_list = torch.load(T5_dir + 'doc_id_list.pkl')
    print('DOC NUMBER: ', len(doc_list))
    print('Data Loaded!')
    gen_doc_embedding_cll(doc_list, bert_model_path, max_seq_length, tmp_dir)
    gen_query_embedding_cll(bert_model_path, max_seq_length, tmp_dir, 'l')


def gen_doc_embedding_cll(doc_list, bert_model_path, max_length, tmp_dir):
    set_seed()
    tokenizer = BertTokenizer.from_pretrained(bert_model_path)
    bert_config = BertConfig.from_pretrained(bert_model_path, output_hidden_states=False)
    bert = BertModel.from_pretrained(bert_model_path, config=bert_config).cuda()
    bert.eval()
    with gzip.open('../data/baseline_data/token_content_50.pkl.gz', 'rb') as f:
        token_dict_all = pickle.load(f)
    doc_emb_f = open(tmp_dir+'doc_bert_cll.emb', 'w')

    batch_size = 256
    times = math.ceil(len(doc_list) / batch_size)
    with torch.no_grad():
        for i in tqdm(range(times)):
            docs = doc_list[i * batch_size:(i + 1) * batch_size]
            input_ids = []
            token_type_ids = []
            attention_mask = []
            for doc in docs:
                if not token_dict_all[doc]:
                    print(doc)
                    continue
                doc_bert_enc = tokenizer.encode_plus(token_dict_all[doc], add_special_tokens=True,
                                            truncation=True, max_length=max_length, padding='max_length')
                input_ids.append(doc_bert_enc['input_ids'])
                token_type_ids.append(doc_bert_enc['token_type_ids'])
                attention_mask.append(doc_bert_enc['attention_mask'])
            doc_bert_input = {'input_ids': torch.LongTensor(input_ids).cuda(),
                              'attention_mask': torch.LongTensor(token_type_ids).cuda(),
                              'token_type_ids': torch.LongTensor(attention_mask).cuda()}
            doc_emb_dict = bert(**doc_bert_input)[1].squeeze(0).cpu()
            for i in range(doc_emb_dict.shape[0]):
                doc_emb_f.write(docs[i])
                for j in range(doc_emb_dict.shape[1]):
                    doc_emb_f.write('\t' + str(float(doc_emb_dict[i][j])))
                doc_emb_f.write('\n')
        doc_emb_f.close()


def gen_query_embedding_cll(bert_model_path, max_seq_length, tmp_dir, fold):
    set_seed()
    query_emb = load_embedding('../data/baseline_data/query.emb')
    token_dict = {}
    tokenizer = BertTokenizer.from_pretrained(bert_model_path)
    max_length = 0
    for query in query_emb.keys():
        sent = tokenizer.tokenize(query)
        token_dict[query] = sent
        max_length = max(max_length, len(sent))
    print('query max length: %d'%(max_length))
    max_length = max_length + 1

    bert_config = BertConfig.from_pretrained(bert_model_path, output_hidden_states=False)
    bert = BertModel.from_pretrained(bert_model_path, config=bert_config).cuda()
    bert.eval()
    pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]
    pad_token_segment_id = 0
    doc_emb_f = open(tmp_dir+'query_bert_cl'+str(fold)+'.emb', 'w')
    with torch.no_grad():
        for query in tqdm(token_dict):
            doc_id = tokenizer.convert_tokens_to_ids(token_dict[query])
            doc_encoding = tokenizer.encode_plus(doc_id, max_length=max_length, add_special_tokens=True, truncation=True)
            doc_input_ids, doc_token_type_ids = doc_encoding['input_ids'], doc_encoding['token_type_ids']
            doc_attention_mask = [1] * len(doc_input_ids)
            padding_length = max_length - len(doc_input_ids)
            doc_input_ids += ([pad_token] * padding_length)
            doc_attention_mask += ([0] * padding_length)
            doc_token_type_ids += ([pad_token_segment_id] * padding_length)
            doc_bert_input = {'input_ids': torch.LongTensor([doc_input_ids]).cuda(),
                              'attention_mask': torch.LongTensor([doc_attention_mask]).cuda(),
                              'token_type_ids': torch.LongTensor([doc_token_type_ids]).cuda()}
            query_bert_emb = bert(**doc_bert_input)[1].squeeze(0).cpu()
            doc_emb_f.write(query)
            for i in range(query_bert_emb.shape[0]):
                doc_emb_f.write('\t' + str(float(query_bert_emb[i])))
            doc_emb_f.write('\n')
    doc_emb_f.close()


def gen_cll_fold_data(pos_k):
    set_seed()
    with gzip.open('../data/attn_data/fold_d.json', 'rb') as f:
        fold_d = pickle.load(f)
    cl_pairl = torch.load('../data/attn_data/cl_pair_pos_'+str(pos_k)+'.pkl')
    jobs = []
    for _fold in range(1, 6, 1):
        out_path_tr = '../data/attn_data/train_list.pos.' + str(_fold) + '.txt'
        p = multiprocessing.Process(target=gen_cll_fold_data_sub, args=(cl_pairl, fold_d[_fold][0], out_path_tr))
        jobs.append(p)
        p.start()
        out_path_de = '../data/attn_data/dev_list.pos.' + str(_fold) + '.txt'
        p = multiprocessing.Process(target=gen_cll_fold_data_sub, args=(cl_pairl, fold_d[_fold][1], out_path_de))
        jobs.append(p)
        p.start()


def gen_cll_fold_data_sub(cl_pairl, fold_d, out_path):
    set_seed()
    out = open(out_path, 'w')
    for train_id in tqdm(fold_d):
        query = cl_pairl[train_id][0]
        for pair in cl_pairl[train_id][1:]:
            out.write(query+'\t'+str(len(pair[0])))
            for i in range(len(pair[0])):
                out.write('\t'+pair[0][i])
            for i in range(len(pair[1])):
                out.write('\t'+pair[1][i])
            out.write('\n')

            out.write(query + '\t' + str(len(pair[0])))
            for i in range(len(pair[0])):
                out.write('\t' + pair[0][i])
            for i in range(len(pair[2])):
                out.write('\t' + pair[2][i])
            out.write('\n')
    out.close()


def doc_change(qd_pairs):
    _rnd = random.Random(0)
    set_seed()
    random_num = int(len(qd_pairs) * 0.25)
    random_positions1 = _rnd.sample(list(range(len(qd_pairs))), random_num)
    random_positions2 = _rnd.sample(list(range(len(qd_pairs))), random_num)
    for i in range(len(random_positions1)):
        tmp = qd_pairs[random_positions1[i]]
        qd_pairs[random_positions1[i]] = qd_pairs[random_positions2[i]]
        qd_pairs[random_positions2[i]] = tmp
    return qd_pairs


if __name__ == '__main__':
    set_seed()
    parser = argparse.ArgumentParser(description='')
    parser.add_argument("--bert_model_path", default="../bert-base-uncased/", type=str, help="")
    parser.add_argument("--bert_emb_len", type=int, default=256, help="")
    args = parser.parse_args()

    pos_k = 3
    T5_dir = '../data/data_content/'
    generate_cl_listpair(50, pos_k, T5_dir)
    generate_cll_train_data(args.bert_model_path, args.bert_emb_len, '../data/attn_data/', T5_dir)
    gen_cll_fold_data(pos_k)
